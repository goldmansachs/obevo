<?xml version="1.0" encoding="ISO-8859-1"?>
<!--

    Copyright 2017 Goldman Sachs.
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.

-->
<document xmlns="http://maven.apache.org/XDOC/2.0"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/XDOC/2.0 http://maven.apache.org/xsd/xdoc-2.0.xsd">
    <properties>
        <title>Existing System Onboarding Guide</title>
    </properties>
    <body>
        <macro name="toc">
            <param name="fromDepth" value="0" />
            <param name="toDepth" value="1" />
        </macro>

        <section name="Full Onboarding Guide for Existing Systems">

            <p>This page outlines how to fully onboard an existing system to DeployAnywhere</p>

            <p>At a high-level, the steps are:
                <ol>
                    <li>Reverse-engineer your schema from production to set as your source code</li>
                    <li>Initialize your production and (optionally) UAT environments with DeployAnywhere</li>
                    <li>Do the recommended final steps mentioned in the
                        <a href="new-onboarding-guide.html">Full Onboarding Guide for New Systems</a>
                    </li>
                </ol>
            </p>

            <p>Note that this implies the production schema is your gold-standard for your code-base (as this is the exact
                schema already defined in production). Thus, you should discard any existing SQLs that you have been
                maintaining as they likely would not meet
                <u>
                    <i>both</i>
                </u>
                of the following criteria:
            <ul>
                <li>Be in the file-per-object format that DeployAnywhere desires</li>
                <li>Be in sync with the actual schema in production</li>
            </ul>

                One exception here is for static data tables (i.e. delete/insert statements for code tables / reference
                data tables / whatever your team calls them). Such code can easily be reusable.
        </p>

            <p>Estimated time effort:
            <ul>
                <li>For steps 1-6, a couple hours for a single instance and single schema of your production
                    environment, which entail:
                    <ul>
                        <li>getting your schema generated from prod</li>
                        <li>re-deploying it to a blank dev db</li>
                        <li>and marking your production schema as &quot;deployed&quot; in the Deploy Log</li>
                    </ul>
                </li>
                <li>At best, your full schema will be deployed to your dev DB; otherwise, most of your objects should be
                    deployable
                </li>
                <li>The rest of the steps (step &gt;= 7) are getting your UAT in line, knowledge transition to your
                    teams, cleanups as necessary. This time would vary depending on your team's situation
                </li>
            </ul>
        </p>
        </section>
        <section name="1) Setup your environment on your local workstation">
            Follow these<a href="setup.html">instructions</a>. Do this from your regular desktop environment for ease
            of use (i.e. no need to run from a controlled environment at this point).
        </section>

        <section name="2) Create a blank schema to test your deployment against">
            It is heavily suggested that that you dedicate a schema just for running the DB scripts. This will have
            multiple uses:
            <ul>
                <li>As a sandbox for you defining your DeployAnywhere project</li>
                <li>For comparing your deployed db against different environments to verify your DB scripts (or the state of your other environments)</li>
                <li>For your continuous builds going forward (i.e. verifying your db scripts are fine as developers change it going forward)</li>
            </ul>
        </section>

        <section name="3) Reverse-Engineering the DDLs from your DB">
            <p>The next step is to reverse-engineer your DB DDLs from your reference environment (likely your production environment)
            to serve as the starting point for your source code</p>

            <p>We have a couple methodologies available for you to use, depending on your DBMS:</p>
            <p><b><u><a href="reverse-engineer-aquadatastudio.html">Option 1 (default): Aqua Data Studio</a></u></b>
                <ul>
                    <li>This is currently the primary methodology for reverse-engineering.</li>
                    <li>It supports all DBMS types</li>
                    <li>We are looking to move to the option below in the future, but it is still in the piloting phase</li>
                </ul>
            </p>
            <p><b><u><a href="reverse-engineer-dbmstools.html">Option 2 (beta - in pilot): vendor-provided tools</a></u></b> (only available for Sybase ASE and DB2)
                <ul>
                    <li>This option uses the tools provided by the vendors</li>
                    <li>This will be the preferred option in the future</li>
                    <li>Today, the feature is in pilot. Reach out to gs-deployanywhere-core if you would like to try this
                        out (particularly if you have a lot of DBMS's to reverse engineer, as this methodology is more
                        automatable than the Aqua Data Studio method
                    </li>
                </ul>
            </p>

            <p>Note that none of the methodologies would include reverse-engineering the grants, as we recommend teams
                to use the <a href="permission-management.html">Permission Management</a> functionality instead
            </p>
        </section>
        <section name="4) Review and modify the system-config.xml file">
            <p>The schema reverse-engineering step will generate a system-config.xml file for you in your output directory
            based on the parameters you provided. Review it and modify it accordingly</p>
            <p>To start with:
                <ul>
                    <li>The source of your reverse-engineering (as specified by your arguments in the previous step) will
                        be named as the "prod" environment in your config, with a separate placeholder environment for your dev</li>
                    <li>Use the placeholder dev environments for your deploys during testing</li>
                    <li>Rename the &lt;schema&gt; entry to become the logical schema name that you want</li>
                    <li>Use the <a href="${kata.product.sourceroot}/src/main/database/system-config.xml">kata system-config.xml file</a> as a further sample to model from if needed</li>
                </ul>
            </p>
        </section>
        <section name="5) (optional) Import your static data as CSV or inserts">
            <h1>How To Generate the CSV Files</h1>
            <source><![CDATA[## required args:
    %DEPLOYANY_HOME%\bin\deploy.bat DBREVENG -mode data -inputDir %s -outputDir %s -dbType %s -dbHost %s -dbPort %d -dbSchema %s -username %s -password %s

## optional args:
# 1) you can add this for DB2
    -dbServer &lt;serverName&gt; # for DB2 databases that require a server parameter
# 2) one of the following is required, but not both
    -tables table1,table2,table3,... # comma-separated list of tables to reverse-engineer
  or
    -inputDir dir # dir containing a file static-data-tables.txt that has the list of tables, one per line (you can use this in case you have many tables that are hard to pass in by command line)

# 3) If you want to automatically detect certain columns as &quot;updateTime columns&quot; and generate the CSV files as such, use this arg. The &quot;updateTime column&quot; concept is explained below
    -updateTimeColumns col1,col2]]></source>
        <p>Clarification for certain params:</p>
        <ul>
            <li>-outputDir will have the CSV files written there</li>
            <li>-dbType should be SYBASE<i>IQ, SYBASE</i>ASE, DB2
            </li>
            <li>-updateTimeColumns:
                <ul>
                    <li>Often times when we have static data tables, we'd denote a field like updateTime for audit
                        purposes, i.e. to indicate when the row was added or inserted
                    </li>
                    <li>But if we define the static data in your source code, then you can't truly add the updateTime
                        column at the same time, as that will get deployed later
                    </li>
                    <li>Hence, we allow users to define certain columns as &quot;updateTime columns&quot;, such that the
                        tool will automatically set this value to the current time whenever the row is inserted or
                        updated
                    </li>
                    <li>To do this, you would add the following metadata tag at the start of your CSV file: (the value
                        is singular: only 1 column can be denoted as such)
                    </li>
                </ul>
            </li>
        </ul>
        <p>//// METADATA updateTimeColumn=&quot;col1&quot; ...</p>
        <ul>
            <li>In terms of the reverse-engineering:
                <ul>
                    <li>The arg defined in the reverse engineering is to specify which columns you expect to be used as
                        such, as teams may have certain conventions, e.g. timeUpdated, updTmstmp, ...
                    </li>
                    <li>The reveng arg is plural in case you happen to have multiple such columns</li>
                </ul>
            </li>
        </ul>
            <h1>Generating the data as Insert statements</h1>
        <p>If you can't or don't want to maintain your static data as CSVs, and want to keep it as inserts, you can use
            Aqua to do this (i.e. Tools -&gt; Export Data), or define these files manually, or whatever other method
            that you have
        </p>
        </section>
        <section name="6) Execute a deployment to your blank schema successfully">
        <p>Use the deployment command as mentioned earlier to deploy your newly-created files into the test schema you
            setup in step 2 earlier <b>including adding the -onboardMode flag to help w/ the onboarding</b>.
            The point of the deployment is so we can verify that the scripts are valid.
        </p>
        <p>Please perform a successful deployment before proceeding to the next step</p>
        <source>%DEPLOYANY_HOME%\bin\deploy.bat DEPLOY -sourcePath src/main/database -env test -onboardingMode</source>
            <p>In DeployAnywhere, we strove to make the reverse-engineering as smooth as possible, so hopefully it will
            be successful on the first try. But for various reasons, it may not. Hence, you should iteratively deploy,
            review the logs, fix problems, and redeploy until all issues are fixed.
        </p>
            <subsection name="Explaining the -onboardingMode flag">
                <p>As the deployment described here will be an iterative process to
                find and then resolve issues, it becomes important to easily identify exceptions. The onboarding mode
                    helps via the following:
                </p>
                <p>1) Any files that cause exceptions will get moved into either the /exceptions or /dependentOnExceptions
                    folders, and will have a separate ".exceptions" file created containing the exceptions you have.
                    <ul>
                        <li>/exceptions gets created for normal cases.</li>
                        <li>/dependentOnExceptions is created if we detect that
                            the exception was caused due to an earlier exception on another object (i.e. something already in
                            /exceptions
                        </li>
                    </ul>
                    2) Upon correction of SQL files and subsequent rerun, files will get moved back to the home folder
                </p>
            </subsection>
            <subsection name="Onboarding workflow">
            <p>
                Your workflow then specifically becomes the following:
                <ul>
                    <li>1) Run a deployment using -onboardingMode with your reverse-engineered DDLs</li>
                    <li>2) Fix the issues you see in the objects stored in /exceptions, whether by fixing SQLs (see notes
                        below), fixing the DB environment itself, or deleting the file and choosing not to maintain it.</li>
                    <li>(leave the "dependentOnExceptions" alone for now)</li>
                    <li>3) Rerun the deployment w/ the -onboardingMode command. You should expect the /exceptions
                    objects you just fixed to be successfully deployed. This means the objects in /dependentOnExceptions
                    now have a chance to succeed. If they do, they will get moved to the regular folder. If not, they
                    will get moved to the /exceptions folder</li>
                    <li>4) Go back to step 2 and repeat steps 2-4 until there are no more exceptions during your run</li>
                    <li>5) Do a final full-deploy (i.e. cleanEnvironment, followed by deploy) to help ensure that
                    the SQLs are actually valid and compatible w/ all environments</li>
                </ul>
            </p>
            </subsection>
            <subsection name="Common problems encountered + resolution steps">
        <ul>
            <li>Objects may refer to other non-existent objects (e.g. views or stored procedures referring to tables
                that no longer exist). This can particularly happen for long-lived systems
                <ul>
                    <li>A suggestion here is to do a grep on the log file of the deployment that you tried for the SQL
                        error that indicates a reference to an object that doesn't exist (e.g. for Sybase, the message
                        is &quot;nested exception is com.sybase.jdbc3.jdbc.SybSQLException: ArchivingCandidate not
                        found. Specify owner.objectname or use sp<i>help to check whether the object exists (sp</i>help
                        may produce lots of output)&quot;. Hence, you can try grepping for &quot;not found&quot;
                    </li>
                    <li>Once you get that list, send it out to your team (possibly w/ the object that referred to it) to
                        see if these objects are actually needed. Likely, you should be able to delete these objects
                        (definitely delete them from your source code, and drop them in production prior to onboarding)
                    </li>
                </ul>
            </li>
            <li>Views/stored procedures/triggers that are referring to objects in other schemas. Depending on your
                situation, you may either:
                <ul>
                    <li>need to tokenize the schema name in case it is different on your dev env vs. production (e.g.
                        account<i>master</i>dev vs. accountmaster)
                    </li>
                    <li>see that the dependent object exists in prod but not dev. In that case, look to get that schema
                        cleaned up (potentially via DeployAnywhere? You can do that as a separate project)
                    </li>
                </ul>
            </li>
            <li>Circular dependencies in stored procedures that the default algorithm could not detect (maybe due to
                comments in your files). In these cases, add the includeDependencies/excludeDependencies as needed (see
                earlier parts in this doc for reference)
            </li>
            <li>(For Sybase ASE) The stored procedure may have been relying on a pre-existing temp table (i.e. the
                client app creates the temp table, populates the data, then calls the SP that uses the temp table).
                <ul>
                    <li>In this case, you must create the temp table before creating the SP. Though the temp table won't
                        exist outside the connection, the SP will remember the name and structure, and thus expect it to
                        be created when called
                    </li>
                    <li>To handle this case, just add the &quot;create temp table&quot; statement in your SP file prior
                        to the &quot;create procedure&quot; statement, and then add a &quot;drop temp table&quot;
                        statement after the actual SP sql (all within the same file)
                        <ul>
                            <li>See the example below for reference</li>
                        </ul>
                    </li>
                </ul>
            </li>
        </ul>
                <source><![CDATA[create table #MyTemp (
    FieldA varchar(32),
    FieldB int
)
GO
create proc SpWithTemp (@MaxCount int) as
begin
    select * from #MyTemp

    // Do whatever processing you need
end
GO
drop table #MyTemp /* cleanup */
GO]]></source>
        <ul>
            <li>For the staticData loads, a couple common issues may arise:
                <ul>
                    <li>If the table does not have a primary key, then the CSV mode will not work, as mentioned earlier
                        in the docs. Either add the PK to the table, or switch to the delete/insert mode
                    </li>
                    <li>If your data includes a backslash (i.e. ), then you need to convert it to escape it w/ another
                        quote (i.e. \\ ). This is because of a bug w/ the underlying CSV reader implementation
                        <a class="externalLink" href="http://opencsv.sourceforge.net/">Opencsv</a>
                        (which otherwise is truly an excellent library!)
                    </li>
                </ul>
            </li>
        </ul>
        <p>If you run into any other kinds of problems that you cannot seem to debug, feel free to reach out to the
            DeployAnywhere team
        </p>
            </subsection>
        </section>
        <section name="7) Marking your files as 'already deployed' in production">
        <p>Once you've gotten your DDLs into a good shape (i.e. doing the reverse engineering, deploying your code
            against the blank schema, then comparing that deployed schema against prod via Aqua to verify correctness),
            then essentially, you can assume that your prod db have been deployed via deployanywhere
        </p>
        <p>But - the audit table does not exist yet in production. You will have to seed it w/ the audit trail such that
            DeployAnywhere can deploy any new changes going forward
        </p>
        <p>To do this, run the INIT command</p>
        <source>%DEPLOYANY_HOME%\bin\deploy.bat INIT -sourcePath src/main/database</source>
        <p>Do the same for the rest of your environments (e.g. qa, uat) if needed</p>
        <p>In case you have some ddls in uat but not production, the suggestion is to:</p>
        <ul>
            <li>first create baseline DDLs based off prod</li>
            <li>then run init-env for prod</li>
            <li>then add your migration script in your source code for your other envs, e.g. uat</li>
            <li>then run init-env for the other env, e.g. uat</li>
            <li>repeat if you have yet another layer of changes (e.g. dev or qa)</li>
        </ul>
        </section>
        <section name="8) Complete the Adoption">
        <p>The above steps are easy - doing the final &quot;production&quot; touches is the harder part, IMO. The time
            and effort to do this would vary depending on your team situation
        </p>
            <subsection name="Apply this to your other environments">
                <h4>UATs</h4>
        <p>You likely will have a number of dev/qa/uat environments already up and running. We'd need to onboard these
            to DeployAnywhere too, and it is possible that these environments are not in sync with production (hence,
            they are out of sync with the db scripts you just created)
        </p>
        <p>You have 2 options to work around this:</p>
        <p>1) Wipe away the UAT environment and recreate (whether via devsync or via DeployAnywhere -cleanFirst
            invocation)
        </p>
        <p>This is the easy option if you don't have any ongoing UATs or can easily start over</p>
        <p>2) Manually compare and apply any changes to the UAT environment, and then apply the &quot;INIT&quot; command
            mentioned in step 7
        </p>
        <p>In the case that you cannot just start afresh, you may need to do manual reconciliations and comparisons to
            get your UAT in line. This is not that great an option, but depending on your team's situation, it may be
            needed
        </p>
        <p>To compare the schemas, use the Aqua Data Studio schema comparison tool or the schema comparison tool
            provided by DeployAnywhere (will document this soon)
        </p>
        <p>To compare the static data, see the following subsection...</p>
                <h4>Comparing the Data across different databases</h4>
        <p>When defining your static data files, it may help to do comparisons between different databases (e.g. prod
            vs. uat, your deployed code vs. prod) in case you are starting from a very unmanaged state (i.e. no static
            data files checked in, not sure of the states your various dbs are in)
        </p>
        <p>To do that - run the following command. It will let you do compare a list of tables that you define against
            the dbs that you define (you can define any number of 2-way compares). The tool will output the differences
            in Excel files (a summary report defining the overall differences, plus detailed reports for cases where
            content exists in either side of a comparison, but there are differences)
        </p>
        <source>j:\cc\deployanywhere\bin\deploy.bat DBDATACOMPARE -configFile H:\mydir\costarRepoComparisonConfig.properties -outputDir H:\mycomparisons\</source>
        <p>Config File Example: <a href="files/dbComparisonConfigExample.properties">dbComparisonConfigExample.properties</a></p>
        <p>Format:</p>
        <ul>
            <li>tables.include -- tables you want to check (you must define this)</li>
            <li>tables.exclude -- tables you don't want to check (you can leave blank if you choose --
                this is more of a convenience in case you were incrementally working on the comparison and wanted )
            </li>
            <li>comparisons -- define the 2-way comparisons that you want to do. E.g. the string &#xe2;&#x80;&#x9c;dev1,uat;uat,prod1;dev1,prod2&#xe2;&#x80;&#x9d;
                will do the following comparisons:
                <ul>
                    <li>dev1 vs. uat</li>
                    <li>uat vs. prod1</li>
                    <li>dev1 vs. prod2</li>
                    <li>Then you have an entry for each of the data sources that you define, and you must define the
                        schema/url/username/password/driverClass for each
                    </li>
                </ul>
            </li>
        </ul>
            </subsection>
            <subsection name="Cleanup old/unmaintained objects as you see fit">
            <ul>
                <li>Note - we ultimately are planning on a utility JIRATOREPLACE-DEPLOYANY-98 to cleanup any objects that are in the environment and not in the source code. Till then, you can try the following manual cleanup options.
                </li>
                <li>Removing backup tables from production (e.g. Transaction<i>bkup, Transaction</i>20130101)
                    <ul>
                        <li>The suggestion is that you do not include these tables into your code base - delete them before
                            you do the INIT command (if you already did the INIT command, you can just run it again after
                            your changes). Drop these manually from production (I'd do this manually as the tool currently
                            does not support dropping tables - better safe than sorry from a tooling perspective! though we
                            will eventually support this)
                        </li>
                    </ul>
                </li>
                <li>Removing unused views/sps
                    <ul>
                        <li>In this case, I &quot;would&quot; keep these in your DeployAnywhere codebase, as after your init
                            command, you can then delete these in a controlled manner. (I'm less worried about the risk of
                            dropping stored procedures, as you can easily add them back)
                        </li>
                    </ul>
                </li>
            </ul>
            </subsection>

            <subsection name="See rest of recommended steps...">
                Follow the rest of the guidance on "More Recommended Steps" in
                <a href="new-onboarding-guide.html">Full Onboarding Guide for New Systems</a>
            </subsection>

        </section>
    </body>
</document>